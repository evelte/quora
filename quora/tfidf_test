import pandas as pd
import os
import quora
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk, string


df = pd.read_pickle(os.path.join(quora.root, 'data', 'corpus.pkl'))
corpus = df.values[0:100]

# nltk.download('punkt')
stemmer = nltk.stem.porter.PorterStemmer()
remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)

def stem_tokens(tokens):
    print(tokens)
    return [stemmer.stem(item) for item in tokens]

def normalize_text(text):
    print(text)
    print(stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map))))
    return stem_tokens(nltk.word_tokenize(text.lower().translate(remove_punctuation_map)))


vectorizer = TfidfVectorizer(tokenizer=normalize_text, stop_words='english')

# run this on complete corpus and store tfidf vector for later use


def cosine_sim(q1, q2):
    tfidf = vectorizer.fit_transform([q1, q2])
    return (tfidf * tfidf.T).A[0, 1]


print(cosine_sim('the little bird lives', 'the little bird is alive'))
print(cosine_sim('is the bird alive?', 'is the bird death?'))
print(cosine_sim('What is the step by step guide to invest in share market in india?',
                 'What is the step by step guide to invest in share market?'))

from quora.wordnet_test import symmetric_sentence_similarity

a, b, c = symmetric_sentence_similarity('What is the step by step guide to invest in share market in india?',
                 'What is the step by step guide to invest in share market?')

print(a)
print(b)
print(c)